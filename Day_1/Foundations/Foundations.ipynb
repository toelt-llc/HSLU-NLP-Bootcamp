{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "üó£ How can we incorporate textual data in Machine Learning Algorithms?\n",
    "\n",
    "ü§î What are the Machine Learning models dedicated to language-related tasks?\n",
    "\n",
    "Examples:\n",
    "\n",
    "- E-mail filtering (legitimate e-mail vs. spam)\n",
    "- Sentiment analysis\n",
    "- Chatbots\n",
    "- Voice/speech recognition\n",
    "- Smart assistants\n",
    "- Language translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is NLP?**\n",
    "\n",
    "Natural Language Processing (NLP) is a subfield of linguistics, computer science and artificial intelligence concerned with the interactions between computers and human language - in particular how to program computers to process and analyze large amounts of natural language data (speech and text).\n",
    "\n",
    "Source: [Wikipedia](https://en.wikipedia.org/wiki/Natural_language_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plan**\n",
    "1. Text Preprocessing\n",
    "2. Vectorizing\n",
    "3. NLP Modeling: Naive Bayes Classifier\n",
    "4. Topic Modeling: the Latent Dirichlet Allocation Algorithm (LDA) (Unsupervised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![plan](pics/plan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Preprocessing\n",
    "For any Machine Learning algorithm, data preprocessing is crucial, and this remains true for algorithms dealing with text\n",
    "\n",
    "‚úçÔ∏è Text preprocessing is quite different from numerical preprocessing. The most common preprocessing tasks for textual data are:\n",
    "\n",
    "- lowercase\n",
    "- dealing with numbers, punctuation, and symbols\n",
    "- splitting\n",
    "- tokenizing\n",
    "- removing \"stopwords\"\n",
    "- lemmatizing\n",
    "\n",
    "<br>\n",
    "\n",
    "### üíª üßπ Basic cleaning with Python core string operations\n",
    "When you have some unstructured text, you can already clean it with some Python built-in string operations\n",
    "\n",
    "<br>\n",
    "\n",
    "üíª ‚úÇÔ∏è [strip](https://docs.python.org/3/library/stdtypes.html#str.strip) (1/2)\n",
    "\n",
    "`strip` removes all the whitespaces at the beginning and the end of a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['   Bonjour, comment ca va ?     ',\n",
       " '    Heyyyyy, how are you doing ?   ',\n",
       " '        Hallo, wie gehts ?     ']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    '   Bonjour, comment ca va ?     ',\n",
    "    '    Heyyyyy, how are you doing ?   ',\n",
    "    '        Hallo, wie gehts ?     '\n",
    "]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bonjour, comment ca va ?',\n",
       " 'Heyyyyy, how are you doing ?',\n",
       " 'Hallo, wie gehts ?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[text.strip() for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª ‚úÇÔ∏è strip (2/2)\n",
    "\n",
    "You can also specify a \"list\" of characters (in the form of a single and unordered string) to be removed at the beginning and at the end of a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"abcd Who is abcd ? That's not a real name!!! abcd\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"abcd Who is abcd ? That's not a real name!!! abcd\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Who is abcd ? That's not a real name!!! \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.strip('bdac')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª üîÑ [replace](https://docs.python.org/3/library/stdtypes.html#str.replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love koalas, koalas are the cutest animals on Earth.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I love koalas, koalas are the cutest animals on Earth.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love pandas, pandas are the cutest animals on Earth.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.replace(\"koala\", \"panda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª üìè [split](https://docs.python.org/3/library/stdtypes.html#str.split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"linkin park / metallica /red hot chili peppers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['linkin park ', ' metallica ', 'red hot chili peppers']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª üî° Lowercase\n",
    "\n",
    "Text modeling algorithms are case-sensitive (the capitalization of words carries meanings and contexts). Two words need to have the same casing to be considered equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i LOVE football sO mUch. FOOTBALL is my passion. Who else loves fOOtBaLL ?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"i LOVE football sO mUch. FOOTBALL is my passion. Who else loves fOOtBaLL ?\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love football so much. football is my passion. who else loves football ?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª üî¢ Numbers\n",
    "\n",
    "‚úÖ We can (and often should) remove numbers during the text preprocessing steps, especially for:\n",
    "\n",
    "- text clustering\n",
    "- collecting keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i do not recommend this restaurant, we waited for so long, like 30 minutes, this is ridiculous'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"i do not recommend this restaurant, we waited for so long, like 30 minutes, this is ridiculous\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i do not recommend this restaurant, we waited for so long, like  minutes, this is ridiculous'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text = ''.join(char for char in text if not char.isdigit())\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª ‚ùóÔ∏è‚ùìPunctuation and Symbols\n",
    "\n",
    "Punctuation like \".?!\" and symbols like \"@#$\" are not useful for topic modeling.\n",
    "\n",
    "Punctuation is barely used properly on social media platforms.\n",
    "\n",
    "Warning: you might want to keep punctuation and symbols for authorship attribution (Authorship attribution is the task of identifying the author of a given text.)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love bubble tea! OMG so #tasty @channel XOXO @$ ^_^ '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I love bubble tea! OMG so #tasty @channel XOXO @$ ^_^ \"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string # \"string\" module is already installed with Python\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love bubble tea OMG so tasty channel XOXO   '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for punctuation in string.punctuation:\n",
    "    text = text.replace(punctuation, '')\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª Combination: `strip` + `lowercase` + `numbers` + `punctuation/symbols`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"   I LOVE Pizza 999 @^_^\",\n",
    "    \"  Study is amazing, take care - 666\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cleaning(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit())\n",
    "\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '')\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i love pizza', 'study is amazing take care']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned = [basic_cleaning(sentence) for sentence in sentences]\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª üîç Removing Tags with [RegEx](https://regexr.com/)\n",
    "\n",
    "We can remove HTML tags using RegEx:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello HSLU!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"<head><body>Hello HSLU!</body></head>\"\"\"\n",
    "cleaned_text = re.sub('<[^<]+?>','', text)\n",
    "\n",
    "print (cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract e-mail addresses from a text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\w'\n",
      "/var/folders/5t/9y469y05463dj441xyl0t6_80000gp/T/ipykernel_37042/3968755570.py:8: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  re.findall('[\\w.+-]+@[\\w-]+\\.[\\w.-]+', txt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['darkvador@gmail.com', 'batman@outlook.com']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "txt = '''\n",
    "    This is a random text, authored by darkvador@gmail.com\n",
    "    and batman@outlook.com, WOW!\n",
    "'''\n",
    "\n",
    "re.findall('[\\w.+-]+@[\\w-]+\\.[\\w.-]+', txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Cleaning with NLTK\n",
    "\n",
    "Natural Language Toolkit (NLTK) is an NLP library that provides preprocessing and modeling tools for text data\n",
    "\n",
    "üìö [NLTK official website](https://www.nltk.org/)\n",
    "\n",
    "üõ† [Installation Documentation](https://www.nltk.org/install.html)\n",
    "\n",
    "### üíª üß© Tokenizing\n",
    "\n",
    "Tokenizing is essentially splitting a sentence, a paragraph, or even an entire piece of text into smaller chunks such as individual words called tokens.\n",
    "\n",
    "\"Natural Language Processing\"  ‚Üí   [\"Natural\",\"Language\",\"Processing\"]\n",
    "\n",
    "üìö [nltk.tokenize](https://www.nltk.org/api/nltk.tokenize.html)\n",
    "\n",
    "üîÖ Here is a quote from Aristotle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is during our darkest moments that we must focus to see the light'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'It is during our darkest moments that we must focus to see the light'\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/arnaud/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/arnaud/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/arnaud/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/arnaud/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When importing nltk for the first time, we need to also download a few built-in libraries\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'is', 'during', 'our', 'darkest', 'moments', 'that', 'we', 'must', 'focus', 'to', 'see', 'the', 'light']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokens = word_tokenize(text)\n",
    "print(word_tokens) # print displays the words in one line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª üõë Stopwords\n",
    "\n",
    "Stopwords are words that are used so frequently that they don't carry much information, especially for topic modeling\n",
    "\n",
    "NLTK has a built-in corpus of English stopwords that can be loaded and used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # you can also choose other languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a tokenized sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\"i\", \"am\", \"going\", \"to\", \"go\", \"to\", \"the\",\n",
    "        \"club\", \"and\", \"party\", \"all\", \"night\", \"long\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What stopwords could be removed ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'to', 'to', 'the', 'and', 'all']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_removed = [w for w in tokens if w in stop_words]\n",
    "stopwords_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What are the meaningful words in this sentence ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['going', 'go', 'club', 'party', 'night', 'long']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_cleaned = [w for w in tokens if not w in stop_words]\n",
    "tokens_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ What if you are not going to the party?\n",
    "\n",
    "üò± \"not\" is also considered as a stopword\n",
    "\n",
    "‚úÖ Removing stopwords is useful for:\n",
    "\n",
    "- topic modeling\n",
    "\n",
    "‚ùå Dangerous for:\n",
    "\n",
    "- sentiment analysis\n",
    "- authorship attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª üß¨ Lemmatizing\n",
    "Lemmatizing is a technique used to find the root of words, in order to group them by their meaning rather than by their exact form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lemmatizing](pics/stem_lemma.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö [nltk.stem - WordNetLemmatizer](https://www.nltk.org/_modules/nltk/stem/wordnet.html)\n",
    "\n",
    "üëá Look at the following sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'He was RUNNING and EATING at the same time =[. He has a bad habit of swimming after playing 3 hours in the Sun =/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He was RUNNING and EATING at the same time =[. He has a bad habit of swimming after playing 3 hours in the Sun =/'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üóì Let's apply the following steps:\n",
    "\n",
    "- Basic cleaning\n",
    "- Tokenizing\n",
    "- Removing stopwords (if not doing sentiment analysis!)\n",
    "- Lemmatizing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üßπ Step 1: Basic Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he was running and eating at the same time  he has a bad habit of swimming after playing  hours in the sun'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_sentence = basic_cleaning(sentence)\n",
    "cleaned_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª üß© Step 2 : Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'was', 'running', 'and', 'eating', 'at', 'the', 'same', 'time', 'he', 'has', 'a', 'bad', 'habit', 'of', 'swimming', 'after', 'playing', 'hours', 'in', 'the', 'sun']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence = word_tokenize(cleaned_sentence)\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üõë Step 3: Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'eating', 'time', 'bad', 'habit', 'swimming', 'playing', 'hours', 'sun']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence_no_stopwords = [w for w in tokenized_sentence if not w in stop_words]\n",
    "print(tokenized_sentence_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª üß¨ Step 4: Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö [WordNetLemmatizer](https://www.nltk.org/_modules/nltk/stem/wordnet.html)\n",
    "\n",
    "[Lemmatization with NLTK](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Lemmatizing the verbs\n",
    "verb_lemmatized = [\n",
    "    WordNetLemmatizer().lemmatize(word, pos = \"v\") # v --> verbs\n",
    "    for word in tokenized_sentence_no_stopwords\n",
    "]\n",
    "\n",
    "# 2 - Lemmatizing the nouns\n",
    "noun_lemmatized = [\n",
    "    WordNetLemmatizer().lemmatize(word, pos = \"n\") # n --> nouns\n",
    "    for word in verb_lemmatized\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Lemmatizing is useful for:\n",
    "\n",
    "- topic modeling\n",
    "- sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text - Takeaways\n",
    "\n",
    "First of all, we can perform some pre-cleaning operations on the pieces of text of a corpus using Python built-in functions such as:\n",
    "\n",
    "- ‚úÇÔ∏è strip\n",
    "- üîÑ replace\n",
    "- üìè split\n",
    "- üî° lowercase\n",
    "- üî¢ removing numbers\n",
    "- ‚ùóÔ∏è removing punctuation and symbols\n",
    "\n",
    "Next, we can apply preprocessing techniques to prepare the pieces of text for NLP algorithms\n",
    "\n",
    "- üß© Tokenizing\n",
    "- üõë Removing stopwords\n",
    "- üß¨ Lemmatizing\n",
    "\n",
    "\n",
    "ü§î Now that the text is preprocessed, how can it be analyzed by Machine Learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vectorizing\n",
    "\n",
    "ü§ñ Machine Learning algorithms cannot process raw text, as it needs to be converted into numbers first\n",
    "\n",
    "**Vectorizing** = the process of converting raw text into a numerical representation\n",
    "\n",
    "There are multiple vectorizing techniques. Among them, we will present:\n",
    "\n",
    "- `Bag-of-Words`\n",
    "- `Tf_idf`\n",
    "- `N-grams`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vectorization](pics/vectorization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Bag-of-Words representation\n",
    "\n",
    "**Bag-of-Words representation(BoW)** is one of the most simple and effective ways to represent text for Machine Learning models.\n",
    "\n",
    "When using this representation, we are simply counting how often each word appears in each document of a corpus. \n",
    "\n",
    "The count for each word becomes a feature:\n",
    "\n",
    "![example_bow](pics/example_bow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª `CountVectorizer`\n",
    "\n",
    "In Scikit-Learn, there is a tool called `CountVectorizer` to generate bag-of-words representations of a set of texts\n",
    "\n",
    "üëâ `CountVectorizer` converts a collection of text documents into a matrix of token counts\n",
    "\n",
    "üìö [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "\n",
    "üëá Look at the following sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'the young dog is running with the cat',\n",
    "    'running is good for your health',\n",
    "    'your cat is young',\n",
    "    'young young young young young cat cat cat'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the CountVectorizer to generate a Bag-of-Words representation of these four sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 1, 1, 2, 1, 1, 0],\n",
       "       [0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1],\n",
       "       [3, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "X = count_vectorizer.fit_transform(texts)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î Can you guess which column represents which word?\n",
    "\n",
    "üî• As soon as the `CountVectorizer` is fitted to the text, you can retrieve all the words seen with `get_feature_names_out()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cat', 'dog', 'for', 'good', 'health', 'is', 'running', 'the',\n",
       "       'with', 'young', 'your'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>for</th>\n",
       "      <th>good</th>\n",
       "      <th>health</th>\n",
       "      <th>is</th>\n",
       "      <th>running</th>\n",
       "      <th>the</th>\n",
       "      <th>with</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the young dog is running with the cat</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>running is good for your health</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your cat is young</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>young young young young young cat cat cat</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           cat  dog  for  good  health  is  \\\n",
       "the young dog is running with the cat        1    1    0     0       0   1   \n",
       "running is good for your health              0    0    1     1       1   1   \n",
       "your cat is young                            1    0    0     0       0   1   \n",
       "young young young young young cat cat cat    3    0    0     0       0   0   \n",
       "\n",
       "                                           running  the  with  young  your  \n",
       "the young dog is running with the cat            1    2     1      1     0  \n",
       "running is good for your health                  1    0     0      0     1  \n",
       "your cat is young                                0    0     0      1     1  \n",
       "young young young young young cat cat cat        0    0     0      5     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vectorized_texts = pd.DataFrame(\n",
    "    X.toarray(),\n",
    "    columns = count_vectorizer.get_feature_names_out(),\n",
    "    index = texts\n",
    ")\n",
    "\n",
    "display(vectorized_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware that there are some limitations when it comes to the bag-of-words representation:\n",
    "\n",
    "‚ùå A BoW does NOT take into account the order of the words  ‚Üí   hence the name `\"Bag of Words\"`\n",
    "\n",
    "‚ùå A BoW does NOT take into account a document's length  ‚Üí   `Tf-idf` to the rescue\n",
    "\n",
    "‚ùå A BoW does NOT capture document context  ‚Üí   `N-gram` to the rescue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. `Tf-idf` Representation\n",
    "\n",
    "Term Frequency (`tf`) & `CountVectorizer`\n",
    "\n",
    "*Idea: The more often a word appears in a document relative to others, the more likely it is that it will be important to this document*\n",
    "\n",
    "Example: if the word elections appears relatively frequently in a document, it is obvious that this document deals with politics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency of a word $x$ in a document $d$ is called **term frequency**, and is denoted by:\n",
    "\n",
    "$\n",
    "TF_{x,d} = \\dfrac{\\text{Number of times term } x \\text{ appears in document } d}{\\text{Total number of terms in the document}}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì In our last example, could we compute $tf_{young.document4}$ ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>for</th>\n",
       "      <th>good</th>\n",
       "      <th>health</th>\n",
       "      <th>is</th>\n",
       "      <th>running</th>\n",
       "      <th>the</th>\n",
       "      <th>with</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the young dog is running with the cat</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>running is good for your health</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your cat is young</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>young young young young young cat cat cat</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           cat  dog  for  good  health  is  \\\n",
       "the young dog is running with the cat        1    1    0     0       0   1   \n",
       "running is good for your health              0    0    1     1       1   1   \n",
       "your cat is young                            1    0    0     0       0   1   \n",
       "young young young young young cat cat cat    3    0    0     0       0   0   \n",
       "\n",
       "                                           running  the  with  young  your  \n",
       "the young dog is running with the cat            1    2     1      1     0  \n",
       "running is good for your health                  1    0     0      0     1  \n",
       "your cat is young                                0    0     0      1     1  \n",
       "young young young young young cat cat cat        0    0     0      5     0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$tf_{young, document4} = \\dfrac{5 \\text{ counts of \"young\"}}{8 \\text{ total words}} = 0.625 $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document Frequency (`df`)\n",
    "\n",
    "*Idea: If a word appears in many documents of a corpus, however, it shouldn't be that important to understand a particular document.*\n",
    "\n",
    "Example: on eurosport.com/football, the word \"football\" appears in every article, hence why the word football on this website is an unimportant word!\n",
    "\n",
    "The number of documents $d$ in a corpus containing the word $x$ is called document frequency (df), and is denoted by $df_{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì In our last example, could we compute $df_{cat}$, $df_{young}$, $df_{the}$ ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>for</th>\n",
       "      <th>good</th>\n",
       "      <th>health</th>\n",
       "      <th>is</th>\n",
       "      <th>running</th>\n",
       "      <th>the</th>\n",
       "      <th>with</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Document Frequency</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cat  dog  for  good  health  is  running  the  with  \\\n",
       "Document Frequency    3    1    1     1       1   3        2    1     1   \n",
       "\n",
       "                    young  your  \n",
       "Document Frequency      3     2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute document frequency (DF)\n",
    "document_frequency = (vectorized_texts > 0).sum(axis=0)\n",
    "\n",
    "# Convert DF into a DataFrame format\n",
    "document_frequency = pd.DataFrame([document_frequency], index=[\"Document Frequency\"])\n",
    "\n",
    "# Display the DataFrame\n",
    "display(document_frequency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a word $x$ appears in too many documents of a corpus - i.e. if the document frequency $df_{x}$ is too high - the word $x$ won't help us with topic modeling and should be considered irrelevant.\n",
    "\n",
    "Example: on eurosport.com/football/, the word \"football\" won't help us distinguish two articles, one dealing mainly with strategy and another one talking about referee best practices!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we considered the **relative document frequency** of a word $x$, which can be computed as:\n",
    "\n",
    "$\n",
    "\\dfrac{df_x}{N}\n",
    "$\n",
    "\n",
    "where:\n",
    "- $df_x$ is the number of documents $d$ containing the word $x$,\n",
    "- $N$ is the total number of documents in a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the word \"football\" on Eurosport, we would expect this formula to be close to 1 since the number of docs containing the word \"football\" will probably only be slightly less than the total number of docs (out of 100 maybe only 5 don't have the word \"football\", so we get 95/100)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: A word $x$ in a corpus of texts will be considered important when its **(relative) document frequency** is **low** ‚áî its inverse document frequency $\\dfrac{N}{df_x}$ is high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, if the word \"football\" appears in all the articles it is not very useful for helping us identify between two articles, but if only a few documents contain words like \"concussion\" or \"wellbeing\", (e.g. they appear in 2/100 articles) it will be much more useful in determining the topic of that article (they are probably specifically about player wellfare)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tf-idf Formula**\n",
    "\n",
    "üí° Thus the intuition of the `term frequency - inverse document frequency` approach is to give a high weight to any term which appears frequently in a single document, but not in too many documents of the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight of a word $x$ in a document $d$ is given by:\n",
    "\n",
    "$$\n",
    "w_{x,d} = tf_{x,d} \\times \\left[ \\log \\left( \\frac{N + 1}{df_x + 1} \\right) + 1 \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $tf_{x,d}$ = $ \\dfrac{\\text{Number of occurrences of word } x \\text{ in document } d}{\\text{Total number of words in document } d} $\n",
    "\n",
    "- $df_x$ = Number of documents $d$ containing the word $x$\n",
    "- $N$ = Total number of documents in a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. üíª TfidfVectorizer\n",
    "\n",
    "`raw documents`  ‚Üí   `matrix of tf-idf features`\n",
    "\n",
    "üìö [sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the young dog is running with the cat',\n",
       " 'running is good for your health',\n",
       " 'your cat is young',\n",
       " 'young young young young young cat cat cat']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>for</th>\n",
       "      <th>good</th>\n",
       "      <th>health</th>\n",
       "      <th>is</th>\n",
       "      <th>running</th>\n",
       "      <th>the</th>\n",
       "      <th>with</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.227904</td>\n",
       "      <td>0.357056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227904</td>\n",
       "      <td>0.281507</td>\n",
       "      <td>0.714112</td>\n",
       "      <td>0.357056</td>\n",
       "      <td>0.227904</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.463709</td>\n",
       "      <td>0.463709</td>\n",
       "      <td>0.463709</td>\n",
       "      <td>0.295980</td>\n",
       "      <td>0.365594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.470063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.470063</td>\n",
       "      <td>0.580622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.514496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857493</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat       dog       for      good    health        is   running  \\\n",
       "0  0.227904  0.357056  0.000000  0.000000  0.000000  0.227904  0.281507   \n",
       "1  0.000000  0.000000  0.463709  0.463709  0.463709  0.295980  0.365594   \n",
       "2  0.470063  0.000000  0.000000  0.000000  0.000000  0.470063  0.000000   \n",
       "3  0.514496  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        the      with     young      your  \n",
       "0  0.714112  0.357056  0.227904  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.365594  \n",
       "2  0.000000  0.000000  0.470063  0.580622  \n",
       "3  0.000000  0.000000  0.857493  0.000000  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating the TfidfVectorizer\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Training it on the texts\n",
    "weighted_words = pd.DataFrame(tf_idf_vectorizer.fit_transform(texts).toarray(),\n",
    "                 columns = tf_idf_vectorizer.get_feature_names_out())\n",
    "\n",
    "weighted_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Controlling the vocabulary size**:\n",
    "\n",
    "In every language, there are many words used in everyday vocabulary:\n",
    "\n",
    "- üá¨üáß English: ~20,000 words\n",
    "- üá´üá∑ French: ~20,000 words\n",
    "- üá©üá™ German: ~20,000 words\n",
    "\n",
    "In a document, we can't afford to vectorize every word!\n",
    "\n",
    "We can, however, control the number of words to be vectorized (*curse of dimensionality*!):\n",
    "\n",
    "üëâ Scikit-Learn allows us to customize the `CountVectorizer` and `TfidVecdtorizer` with key parameters to control vocabulary size.\n",
    "\n",
    "üíª Key parameters of `TfidfVectorizer` (and `CountVectorizer`)\n",
    "- `max_df/min_df`\n",
    "- `max_features`\n",
    "\n",
    "üíª `max_df` (resp. `min_df`)\n",
    "\n",
    "*When building the vocabulary, `CountVectorizer` and `TfidfVectorizer` will remove terms which have a document frequency strictly higher (resp. lower) than the given threshold. `max_df` and `min_df` help us building corpus-specific stopwords.*\n",
    "\n",
    "Example: when classifying pieces of text into \"basketball\" or \"football\", the word \"ball\" would appear too often and would be useless for this classification, it would be better to filter it out using `max_df`\n",
    "\n",
    "**How to use these parameters in practice?**\n",
    "\n",
    "`max_df` (`min_df`) can be either a float between 0.0 and 1.0 or an integer\n",
    "\n",
    "- `max_df` (`min_df`) = 0.5  ‚áî   \"ignore terms that appear in more (less) than 50% of the documents\"\n",
    "- `max_df` (`min_df`) = 20  ‚áî   \"ignore terms that appear in more (less) than 20 documents\"\n",
    "\n",
    "By default, `max_df` = 1.0  ‚áî  no \"frequent\" word will be removed\n",
    "\n",
    "By default, `min_df` = 0.0  ‚áî   no \"infrequent\" word will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>for</th>\n",
       "      <th>good</th>\n",
       "      <th>health</th>\n",
       "      <th>is</th>\n",
       "      <th>running</th>\n",
       "      <th>the</th>\n",
       "      <th>with</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Document Frequency</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cat  dog  for  good  health  is  running  the  with  \\\n",
       "Document Frequency    3    1    1     1       1   3        2    1     1   \n",
       "\n",
       "                    young  your  \n",
       "Document Frequency      3     2  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of occurences of each word\n",
    "document_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dog</th>\n",
       "      <th>for</th>\n",
       "      <th>good</th>\n",
       "      <th>health</th>\n",
       "      <th>running</th>\n",
       "      <th>the</th>\n",
       "      <th>with</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the young dog is running with the cat</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>running is good for your health</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your cat is young</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>young young young young young cat cat cat</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           dog  for  good  health  running  \\\n",
       "the young dog is running with the cat        1    0     0       0        1   \n",
       "running is good for your health              0    1     1       1        1   \n",
       "your cat is young                            0    0     0       0        0   \n",
       "young young young young young cat cat cat    0    0     0       0        0   \n",
       "\n",
       "                                           the  with  your  \n",
       "the young dog is running with the cat        2     1     0  \n",
       "running is good for your health              0     0     1  \n",
       "your cat is young                            0     0     1  \n",
       "young young young young young cat cat cat    0     0     0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the CountVectorizer with max_df = 2\n",
    "count_vectorizer = CountVectorizer(max_df = 2) # removing \"cat\", \"is\", \"young\"\n",
    "\n",
    "# Train it\n",
    "X = count_vectorizer.fit_transform(texts)\n",
    "X = pd.DataFrame(\n",
    "    X.toarray(),\n",
    "    columns = count_vectorizer.get_feature_names_out(),\n",
    "    index = texts\n",
    ")\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª max_features\n",
    "\n",
    "By specifying `max_features` = $k$ (k being an integer), the `CountVectorizer` (or the `TfidfVectorizer`) will build a vocabulary that only considers the top $k$ tokens ordered by term frequency across the corpus.\n",
    "\n",
    "**How to use \"max_features\" in practice?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>is</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the young dog is running with the cat</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>running is good for your health</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your cat is young</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>young young young young young cat cat cat</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           cat  is  young\n",
       "the young dog is running with the cat        1   1      1\n",
       "running is good for your health              0   1      0\n",
       "your cat is young                            1   1      1\n",
       "young young young young young cat cat cat    3   0      5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer with the 3 most frequent words\n",
    "count_vectorizer = CountVectorizer(max_features = 3)\n",
    "\n",
    "X = count_vectorizer.fit_transform(texts)\n",
    "X = pd.DataFrame(\n",
    "    X.toarray(),\n",
    "     columns = count_vectorizer.get_feature_names_out(),\n",
    "     index = texts\n",
    ")\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Advantages of the `Tf-idf` representation:\n",
    "\n",
    "- Using relative frequency rather than count is robust to document length\n",
    "- Takes into account the context of the whole corpus\n",
    "\n",
    "‚ùå Disadvantages of the `Tf-idf` representation:\n",
    "\n",
    "- Like the `BoW`, `Tf-idf` does NOT capture the **within-document context** ‚Üí  `N-gram` helps here\n",
    "- Like the `BoW`, the word order is completely disregarded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. `N-grams`\n",
    "\n",
    "Example: the two following sentences have the exact same representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors_movie = [\n",
    "    \"I like the movie but NOT the actors\",\n",
    "    \"I like the actors but NOT the movie\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actors</th>\n",
       "      <th>but</th>\n",
       "      <th>like</th>\n",
       "      <th>movie</th>\n",
       "      <th>not</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I like the movie but NOT the actors</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I like the actors but NOT the movie</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     actors  but  like  movie  not  the\n",
       "I like the movie but NOT the actors       1    1     1      1    1    2\n",
       "I like the actors but NOT the movie       1    1     1      1    1    2"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize the sentences\n",
    "count_vectorizer = CountVectorizer()\n",
    "actors_movie_vectorized = count_vectorizer.fit_transform(actors_movie)\n",
    "\n",
    "# Show the representations in a nice DataFrame\n",
    "actors_movie_vectorized = pd.DataFrame(\n",
    "    actors_movie_vectorized.toarray(),\n",
    "    columns = count_vectorizer.get_feature_names_out(),\n",
    "    index = actors_movie\n",
    ")\n",
    "\n",
    "# Show the vectorized movies\n",
    "actors_movie_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using a `bag-of-words` representation, an efficient way to capture context is to consider:\n",
    "\n",
    "- the count of single tokens (unigrams)\n",
    "- the count of pairs (bigrams), triplets (trigrams), and more generally sequences of $n$ words, also known as `n-grams`\n",
    "\n",
    "Examples:\n",
    "\n",
    "- \"mathematics\" is a unigram (n = 1)\n",
    "- \"machine learning\" is a bigram (n = 2)\n",
    "- \"natural language processing\" is a trigram (n = 3)\n",
    "- \"deep convolutional neural networks\" is a 4-gram (n = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª `ngram_range`\n",
    "\n",
    "In both `CountVectorizer` and `TfidfVectorizer`, you can specify the length of your sequences with the parameter `ngram_range` = (`min_n`, `max_n`).\n",
    "\n",
    "Examples:\n",
    "\n",
    "- ngram_range = (1, 1) üëâ (by default) will only capture the unigrams (single words)\n",
    "- ngram_range = (1, 2) üëâ will capture the unigrams, and the bigrams\n",
    "- ngram_range = (1, 3) üëâ will capture the unigrams, the bigrams, and the trigrams\n",
    "- ngram_range = (2, 3) üëâ will capture the bigrams, and the trigrams but not the unigrams\n",
    "\n",
    "With a unigram vectorization, we couldn't distinguish two sentences with the same words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actors</th>\n",
       "      <th>but</th>\n",
       "      <th>like</th>\n",
       "      <th>movie</th>\n",
       "      <th>not</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I like the movie but NOT the actors</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I like the actors but NOT the movie</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     actors  but  like  movie  not  the\n",
       "I like the movie but NOT the actors       1    1     1      1    1    2\n",
       "I like the actors but NOT the movie       1    1     1      1    1    2"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actors_movie_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " What about a **bigram vectorization**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actors but</th>\n",
       "      <th>but not</th>\n",
       "      <th>like the</th>\n",
       "      <th>movie but</th>\n",
       "      <th>not the</th>\n",
       "      <th>the actors</th>\n",
       "      <th>the movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I like the movie but NOT the actors</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I like the actors but NOT the movie</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     actors but  but not  like the  movie but  \\\n",
       "I like the movie but NOT the actors           0        1         1          1   \n",
       "I like the actors but NOT the movie           1        1         1          0   \n",
       "\n",
       "                                     not the  the actors  the movie  \n",
       "I like the movie but NOT the actors        1           1          1  \n",
       "I like the actors but NOT the movie        1           1          1  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize the sentences\n",
    "count_vectorizer_n_gram = CountVectorizer(ngram_range = (2,2)) # BI-GRAMS\n",
    "actors_movie_vectorized_n_gram = count_vectorizer_n_gram.fit_transform(actors_movie)\n",
    "\n",
    "# Show the representations in a nice DataFrame\n",
    "actors_movie_vectorized_n_gram = pd.DataFrame(\n",
    "    actors_movie_vectorized_n_gram.toarray(),\n",
    "    columns = count_vectorizer_n_gram.get_feature_names_out(),\n",
    "    index = actors_movie\n",
    ")\n",
    "\n",
    "# Show the vectorized movies with bigrams\n",
    "actors_movie_vectorized_n_gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëç The two sentences are now distinguishable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Vectorizing - Takeaways**\n",
    "\n",
    "There are two methods for vectorizing:\n",
    "- `CountVectorizer` (counting)\n",
    "- `TfidfVectorizer` (weighing: take the document length into consideration)\n",
    "\n",
    "The most important parameters of these vectorizers are:\n",
    "- `min_df` (infrequent words)\n",
    "- `max_df` (frequent words)\n",
    "- `max_features` (curse of dimensionality)\n",
    "- `ngram_range` = (`min_n`, `max_n`) (capturing the context of the words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. (Multinomial) Naive Bayes Algorithm\n",
    "\n",
    "The Multinomial Naive Bayes algorithm is a classification algorithm based on Bayes' Theorem in probability theory\n",
    "\n",
    "### 3.1. ‚úâÔ∏è The E-mail Classification Problem\n",
    "\n",
    "üéØ We want to classify e-mails based on their content:\n",
    "- ‚úÖ Normal (N)\n",
    "- üì© Spam (S)\n",
    "\n",
    "ü§î What is the probability that an e-mail containing some specific words be spam?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Approach\n",
    "\n",
    "Mathematically speaking, the probability that an e-mail containing specific words is spam can be denoted by:\n",
    "\n",
    "$\n",
    "P(\\textcolor{red}{S} \\mid x_1, x_2, \\dots, x_k)\n",
    "$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\textcolor{red}{S}$ = \"this e-mail is spam\"\n",
    "- $x_k$ = \"the word $x_k$ appears in this e-mail\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bayes_theorem](pics/bayes_theorem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "P(\\textcolor{red}{S} \\mid x_1, x_2, \\dots, x_k) = \\dfrac{P(x_1, x_2, \\dots, x_k \\mid \\textcolor{red}{S}) \\times P(\\textcolor{red}{S})}{P(x_1, x_2, \\dots, x_k)}\n",
    "$\n",
    "  \n",
    "(Bayes' Theorem)\n",
    "\n",
    "\n",
    "$\n",
    "P(\\textcolor{red}{S} \\mid x_1, x_2, \\dots, x_k) =\n",
    "\\dfrac{P(x_1, x_2, \\dots, x_k \\mid \\textcolor{red}{S}) \\times P(\\textcolor{red}{S})}\n",
    "{P(x_1, x_2, \\dots, x_k \\cap \\textcolor{red}{S}) + P(x_1, x_2, \\dots, x_k \\cap \\textcolor{cyan}{N})}\n",
    "$\n",
    "\n",
    "(Law of Total Probabilities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![total_prob](pics/total_prob.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "P(\\textcolor{red}{S} \\mid x_1, x_2, \\dots, x_k) =\n",
    "\\dfrac{P(x_1, x_2, \\dots, x_k \\mid \\textcolor{red}{S}) \\times P(\\textcolor{red}{S})}\n",
    "{P(x_1, x_2, \\dots, x_k \\mid \\textcolor{red}{S}) \\times P(\\textcolor{red}{S}) + P(x_1, x_2, \\dots, x_k \\mid \\textcolor{cyan}{N}) \\times P(\\textcolor{cyan}{N})}\n",
    "$\n",
    "\n",
    "(Conditional Probability)\n",
    "\n",
    "<br>\n",
    "\n",
    "üëâ Let's focus on a specific term:\n",
    "\n",
    "$\n",
    "P(x_1, x_2, \\dots, x_k \\mid \\textcolor{red}{S})\n",
    "$\n",
    "\n",
    "<br>\n",
    "\n",
    "The Naive Bayes algorithm makes the strong assumption that the words in an e-mail are **conditionally independent**\n",
    "\n",
    "By applying the independence property:\n",
    "\n",
    "$\n",
    "P(x_1, x_2, \\dots, x_k \\mid \\textcolor{red}{S}) =\n",
    "P(x_1 \\mid \\textcolor{red}{S}) \\times P(x_2 \\mid \\textcolor{red}{S}) \\times \\dots \\times P(x_k \\mid \\textcolor{red}{S})\n",
    "$\n",
    "\n",
    "$\n",
    "= \\prod_{i=1}^{k} P(x_i \\mid \\textcolor{red}{S})\n",
    "$\n",
    "\n",
    "<br>\n",
    "\n",
    "üß® In the **Na√Øve Bayes** algorithm, the probability that an e-mail is spam if it contains certain words is given by:\n",
    "\n",
    "**Spam Formula**:\n",
    "\n",
    "$\n",
    "P(\\textcolor{red}{S} \\mid x_1, x_2, \\dots, x_k) =\n",
    "\\dfrac{\n",
    "P(\\textcolor{red}{S}) \\times \\prod_{i=1}^{k} P(x_i \\mid \\textcolor{red}{S})\n",
    "}{\n",
    "P(\\textcolor{red}{S}) \\times \\prod_{i=1}^{k} P(x_i \\mid \\textcolor{red}{S}) +\n",
    "P(\\textcolor{cyan}{N}) \\times \\prod_{i=1}^{k} P(x_i \\mid \\textcolor{cyan}{N})\n",
    "}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üíª **Computational Approach**  \n",
    "\n",
    "Imagine that you have an e-mail inbox with:  \n",
    "\n",
    "- **8** normal e-mails  \n",
    "- **4** spam e-mails  \n",
    "\n",
    "‚ùì What is the probability that an e-mail containing *\"Dear Friend\"* is spam? ‚ùì  \n",
    "\n",
    "$\n",
    "P(\\textcolor{red}{S} \\mid \\text{\"Dear\"}, \\text{\"Friend\"})\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bayes_example](pics/bayes_compute.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability of an E-mail Being Spam if it Contains \"Dear Friend\"\n",
    "\n",
    "The probability of an e-mail being spam given that it contains *\"Dear Friend\"* is:\n",
    "\n",
    "$\n",
    "P(\\textcolor{red}{S} \\mid \\text{\"Dear\"}, \\text{\"Friend\"}) = $\n",
    "\n",
    "$\\dfrac{\n",
    "P(\\textcolor{red}{S}) \\times P(\\text{\"Dear\"} \\mid \\textcolor{red}{S}) \\times P(\\text{\"Friend\"} \\mid \\textcolor{red}{S})\n",
    "}{\n",
    "P(\\textcolor{red}{S}) \\times P(\\text{\"Dear\"} \\mid \\textcolor{red}{S}) \\times P(\\text{\"Friend\"} \\mid \\textcolor{red}{S}) +\n",
    "P(\\textcolor{cyan}{N}) \\times P(\\text{\"Dear\"} \\mid \\textcolor{cyan}{N}) \\times P(\\text{\"Friend\"} \\mid \\textcolor{cyan}{N})\n",
    "}\n",
    "$\n",
    "\n",
    "<br>\n",
    "\n",
    "Substituting values:\n",
    "\n",
    "$\n",
    "= \\dfrac{\\dfrac{4}{8+4} \\times \\dfrac{2}{7} \\times \\dfrac{1}{7}}\n",
    "{\\dfrac{4}{8+4} \\times \\dfrac{2}{7} \\times \\dfrac{1}{7} + \\dfrac{8}{8+4} \\times \\dfrac{8}{17} \\times \\dfrac{5}{17}}\n",
    "$\n",
    "\n",
    "$\n",
    "= \\dfrac{0.0136}{0.0136 + 0.0923} = 0.129 (= 12.9\\%)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smoothing\n",
    "\n",
    "Imagine that you want to compute:\n",
    "\n",
    "$\n",
    "P(\\textcolor{red}{S} \\mid \\text{\"Dear\"}, \\text{\"Lunch\"})\n",
    "¬®$\n",
    "\n",
    "**You will be in trouble**  \n",
    "\n",
    "$\n",
    "P(\\text{\"Dear\"} \\mid \\textcolor{red}{S}) = \\frac{2}{7} = 0.29\n",
    "$\n",
    "\n",
    "$\n",
    "P(\\text{\"Lunch\"} \\mid \\textcolor{red}{S}) = \\frac{0}{7} = 0.00\n",
    "$\n",
    "\n",
    "And when multiplying these probabilities, you will have a **null probability**!  \n",
    "\n",
    "ü§î **How do we overcome this problem with words that don‚Äôt appear in spam e-mails?**  \n",
    "\n",
    "üí° **We can add +1 (or** $\\alpha > 0$ **) to term frequencies.**  \n",
    "\n",
    "**This is called smoothing, and** $\\alpha$ **is the smoothing parameter.**\n",
    "\n",
    "![smooth_example](pics/example_smooth.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Pros and Cons of the NB Algorithm\n",
    "\n",
    "‚úÖ Pros:\n",
    "\n",
    "- Easy to implement\n",
    "- Not an iterative learning process ‚Äî fast!\n",
    "- Works particularly well on text data because it can handle a large vocabulary\n",
    "- Not a parametric model (no $Œ≤$ to learn, no loss function to minimize)\n",
    "\n",
    "‚ùå Cons:\n",
    "\n",
    "- Assumes that the words appearing in a document don't depend on any previous words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. üíª Implementation of the Naive Bayes Algorithm\n",
    "\n",
    "üìö [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "\n",
    "‚úâÔ∏è Let's have a look at a dataset with thousands of e-mails classified either as spam or as a normal e-mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam\n",
       "0  Subject: naturally irresistible your corporate...     1\n",
       "1  Subject: the stock trading gunslinger  fanny i...     1\n",
       "2  Subject: unbelievable new homes made easy  im ...     1\n",
       "3  Subject: 4 color printing special  request add...     1\n",
       "4  Subject: do not have money , get software cds ...     1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data/ham_spam_emails.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5728, 2)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spam\n",
       "0    0.76\n",
       "1    0.24\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(data[\"spam\"].value_counts(normalize = True), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Feature/Target\n",
    "X = data[\"text\"]\n",
    "y = data[\"spam\"]\n",
    "\n",
    "# Pipeline vectorizer + Naive Bayes\n",
    "pipeline_naive_bayes = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    MultinomialNB()\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "cv_results = cross_validate(pipeline_naive_bayes, X, y, cv = 5, scoring = [\"recall\"])\n",
    "average_recall = cv_results[\"test_recall\"].mean()\n",
    "np.round(average_recall,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëç On average, the Naive Bayes algorithm is able to capture almost half of the spam e-mails, which is quite a good performance for a \"naive\" model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. üíª Tuning the Vectorizer and the Naive Bayes Algorithm Simultaneously\n",
    "\n",
    "üö® Different vectorizing hyperparameters will affect the performance of the model. As such, it is important to simultaneously tune the hyperparameters of both the vectorizer and the Naive Bayes model.\n",
    "\n",
    "üí° Remember that all the transformers and estimators of Scikit-Learn can be pipelined!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best Score = 0.9524932488436137\n",
      "Best params = {'multinomialnb__alpha': 0.1, 'tfidfvectorizer__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the grid of parameters\n",
    "parameters = {\n",
    "    'tfidfvectorizer__ngram_range': ((1,1), (2,2)),\n",
    "    'multinomialnb__alpha': (0.1,1)\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline_naive_bayes,\n",
    "    parameters,\n",
    "    scoring = \"recall\",\n",
    "    cv = 5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(data.text,data.spam)\n",
    "\n",
    "# Best score\n",
    "print(f\"Best Score = {grid_search.best_score_}\")\n",
    "\n",
    "# Best params\n",
    "print(f\"Best params = {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Topic Modeling and Latent Dirichlet Allocation üî•\n",
    "\n",
    "üìö This section is based on a research paper called [Latent Dirichlet Allocation](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) [di ÄiÀàkleÀê] published in 2003 in the Journal of Machine Learning by David M. Bei (Columbia), Andrew Y. Ng (Stanford), Michael I. Jordan (Berkeley). \n",
    "\n",
    "\n",
    "üìö If you read [Wikipedia/Latent_Dirichlet_Model](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Model), you will see that there are plenty of parameters and complex probability distributions to deal with\n",
    "\n",
    "üê£ Consider this section an introduction to topic modeling, we will give you:\n",
    "\n",
    "- an intuition about how LDA works\n",
    "- how to use it on some texts\n",
    "\n",
    "\n",
    "### 4.1. What is LDA?\n",
    "\n",
    "`Latent Dirichlet Allocation` is an unsupervised algorithm for finding topics in documents\n",
    "\n",
    "- \"Latent\" = hidden (topics)\n",
    "- \"Dirichlet\" = type of probability distribution\n",
    "    - Document  ‚Üí   collection of topics\n",
    "    - Topic  ‚Üí   collection of tokens/words\n",
    "\n",
    "üìö [sklearn.decomposition.LatentDirichletAllocation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)\n",
    "\n",
    "üëá Consider the following documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I like mangos and oranges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Frogs and turtles live in ponds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kittems and pippies are fluffy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I had a spincah and kiwi smoothie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My kitten loves sttrawberries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           documents\n",
       "0          I like mangos and oranges\n",
       "1    Frogs and turtles live in ponds\n",
       "2     Kittems and pippies are fluffy\n",
       "3  I had a spincah and kiwi smoothie\n",
       "4      My kitten loves sttrawberries"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = pd.DataFrame(['I like mangos and oranges', 'Frogs and turtles live in ponds',\n",
    "                          'Kittems and pippies are fluffy', 'I had a spincah and kiwi smoothie',\n",
    "                          'My kitten loves sttrawberries'], columns=['documents'])\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìë **Inputs**:\n",
    "\n",
    "- Document-term matrix: documents to be converted using a vectorizer\n",
    "- Number of topics: number of topics to be discovered within the documents\n",
    "    - Each \"topic\" consists of a set of unordered words  ‚Üí   **bag-of-words** format\n",
    "- Number of iterations  ‚Üí  LDA is an unsupervised iterative process\n",
    "\n",
    "üéØ **Output**:\n",
    "\n",
    "- Topics across different documents/pieces of text\n",
    "    - These topics can be interpreted as \"non-linear Principal Components\" of the documents in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. üíª Implementation of the LDA\n",
    "\n",
    "üëá Remember our original documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I like mangos and oranges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Frogs and turtles live in ponds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kittems and pippies are fluffy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I had a spincah and kiwi smoothie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My kitten loves sttrawberries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           documents\n",
       "0          I like mangos and oranges\n",
       "1    Frogs and turtles live in ponds\n",
       "2     Kittems and pippies are fluffy\n",
       "3  I had a spincah and kiwi smoothie\n",
       "4      My kitten loves sttrawberries"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. üíª Cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def cleaning(sentence):\n",
    "\n",
    "    # Basic cleaning\n",
    "    sentence = sentence.strip() ## remove whitespaces\n",
    "    sentence = sentence.lower() ## lowercase\n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit()) ## remove numbers\n",
    "\n",
    "    # Advanced cleaning\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '') ## remove punctuation\n",
    "\n",
    "    tokenized_sentence = word_tokenize(sentence) ## tokenize\n",
    "    stop_words = set(stopwords.words('english')) ## define stopwords\n",
    "\n",
    "    tokenized_sentence_cleaned = [ ## remove stopwords\n",
    "        w for w in tokenized_sentence if not w in stop_words\n",
    "    ]\n",
    "\n",
    "    lemmatized = [\n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"v\")\n",
    "        for word in tokenized_sentence_cleaned\n",
    "    ]\n",
    "\n",
    "    cleaned_sentence = ' '.join(word for word in lemmatized)\n",
    "\n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          like mangos oranges\n",
       "1       frog turtle live ponds\n",
       "2       kittems pippies fluffy\n",
       "3        spincah kiwi smoothie\n",
       "4    kitten love sttrawberries\n",
       "Name: documents, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_documents = documents[\"documents\"].apply(cleaning)\n",
    "cleaned_documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. üíª Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fluffy</th>\n",
       "      <th>frog</th>\n",
       "      <th>kittems</th>\n",
       "      <th>kitten</th>\n",
       "      <th>kiwi</th>\n",
       "      <th>like</th>\n",
       "      <th>live</th>\n",
       "      <th>love</th>\n",
       "      <th>mangos</th>\n",
       "      <th>oranges</th>\n",
       "      <th>pippies</th>\n",
       "      <th>ponds</th>\n",
       "      <th>smoothie</th>\n",
       "      <th>spincah</th>\n",
       "      <th>sttrawberries</th>\n",
       "      <th>turtle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fluffy  frog  kittems   kitten     kiwi     like  live     love   mangos  \\\n",
       "0  0.00000   0.0  0.00000  0.00000  0.00000  0.57735   0.0  0.00000  0.57735   \n",
       "1  0.00000   0.5  0.00000  0.00000  0.00000  0.00000   0.5  0.00000  0.00000   \n",
       "2  0.57735   0.0  0.57735  0.00000  0.00000  0.00000   0.0  0.00000  0.00000   \n",
       "3  0.00000   0.0  0.00000  0.00000  0.57735  0.00000   0.0  0.00000  0.00000   \n",
       "4  0.00000   0.0  0.00000  0.57735  0.00000  0.00000   0.0  0.57735  0.00000   \n",
       "\n",
       "   oranges  pippies  ponds  smoothie  spincah  sttrawberries  turtle  \n",
       "0  0.57735  0.00000    0.0   0.00000  0.00000        0.00000     0.0  \n",
       "1  0.00000  0.00000    0.5   0.00000  0.00000        0.00000     0.5  \n",
       "2  0.00000  0.57735    0.0   0.00000  0.00000        0.00000     0.0  \n",
       "3  0.00000  0.00000    0.0   0.57735  0.57735        0.00000     0.0  \n",
       "4  0.00000  0.00000    0.0   0.00000  0.00000        0.57735     0.0  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "vectorized_documents = vectorizer.fit_transform(cleaned_documents)\n",
    "vectorized_documents = pd.DataFrame(\n",
    "    vectorized_documents.toarray(),\n",
    "    columns = vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "vectorized_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 üíª Finding the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"‚ñ∏\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"‚ñæ\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(max_iter=100, n_components=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LatentDirichletAllocation</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\">?<span>Documentation for LatentDirichletAllocation</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LatentDirichletAllocation(max_iter=100, n_components=2)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(max_iter=100, n_components=2)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Instantiate the LDA\n",
    "n_components = 2\n",
    "lda_model = LatentDirichletAllocation(n_components=n_components, max_iter = 100)\n",
    "\n",
    "# Fit the LDA on the vectorized documents\n",
    "lda_model.fit(vectorized_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Document Mixture (of Topics)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic_mixture = lda_model.transform(vectorized_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20123894, 0.79876106],\n",
       "       [0.18588725, 0.81411275],\n",
       "       [0.20123763, 0.79876237],\n",
       "       [0.8049096 , 0.1950904 ],\n",
       "       [0.80490962, 0.19509038]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topic_mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î How could our topic modeling be improved?\n",
    "\n",
    "- by increasing the number of sentences\n",
    "- by increasing the number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic Mixture (of Words)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_mixture = pd.DataFrame(\n",
    "    lda_model.components_,\n",
    "    columns = vectorizer.get_feature_names_out()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fluffy</th>\n",
       "      <th>frog</th>\n",
       "      <th>kittems</th>\n",
       "      <th>kitten</th>\n",
       "      <th>kiwi</th>\n",
       "      <th>like</th>\n",
       "      <th>live</th>\n",
       "      <th>love</th>\n",
       "      <th>mangos</th>\n",
       "      <th>oranges</th>\n",
       "      <th>pippies</th>\n",
       "      <th>ponds</th>\n",
       "      <th>smoothie</th>\n",
       "      <th>spincah</th>\n",
       "      <th>sttrawberries</th>\n",
       "      <th>turtle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.51658</td>\n",
       "      <td>0.514375</td>\n",
       "      <td>0.51658</td>\n",
       "      <td>1.066362</td>\n",
       "      <td>1.066362</td>\n",
       "      <td>0.516589</td>\n",
       "      <td>0.514375</td>\n",
       "      <td>1.066362</td>\n",
       "      <td>0.516589</td>\n",
       "      <td>0.516589</td>\n",
       "      <td>0.51658</td>\n",
       "      <td>0.514375</td>\n",
       "      <td>1.066362</td>\n",
       "      <td>1.066362</td>\n",
       "      <td>1.066362</td>\n",
       "      <td>0.514375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.06077</td>\n",
       "      <td>0.985625</td>\n",
       "      <td>1.06077</td>\n",
       "      <td>0.510988</td>\n",
       "      <td>0.510988</td>\n",
       "      <td>1.060761</td>\n",
       "      <td>0.985625</td>\n",
       "      <td>0.510988</td>\n",
       "      <td>1.060761</td>\n",
       "      <td>1.060761</td>\n",
       "      <td>1.06077</td>\n",
       "      <td>0.985625</td>\n",
       "      <td>0.510988</td>\n",
       "      <td>0.510988</td>\n",
       "      <td>0.510988</td>\n",
       "      <td>0.985625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fluffy      frog  kittems    kitten      kiwi      like      live  \\\n",
       "0  0.51658  0.514375  0.51658  1.066362  1.066362  0.516589  0.514375   \n",
       "1  1.06077  0.985625  1.06077  0.510988  0.510988  1.060761  0.985625   \n",
       "\n",
       "       love    mangos   oranges  pippies     ponds  smoothie   spincah  \\\n",
       "0  1.066362  0.516589  0.516589  0.51658  0.514375  1.066362  1.066362   \n",
       "1  0.510988  1.060761  1.060761  1.06077  0.985625  0.510988  0.510988   \n",
       "\n",
       "   sttrawberries    turtle  \n",
       "0       1.066362  0.514375  \n",
       "1       0.510988  0.985625  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the five most relevant words for each topic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(lda_model, vectorizer, top_words):\n",
    "    # 1. TOPIC MIXTURE OF WORDS FOR EACH TOPIC\n",
    "    topic_mixture = pd.DataFrame(\n",
    "        lda_model.components_,\n",
    "        columns = vectorizer.get_feature_names_out()\n",
    "    )\n",
    "\n",
    "    # 2. FINDING THE TOP WORDS FOR EACH TOPIC\n",
    "    ## Number of topics\n",
    "    n_components = topic_mixture.shape[0]\n",
    "\n",
    "    ## Top words for each topic\n",
    "    for topic in range(n_components):\n",
    "        print(\"-\"*10)\n",
    "        print(f\"For topic {topic}, here are the the top {top_words} words with weights:\")\n",
    "\n",
    "        topic_df = topic_mixture.iloc[topic]\\\n",
    "            .sort_values(ascending = False).head(top_words)\n",
    "\n",
    "        print(round(topic_df,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "For topic 0, here are the the top 5 words with weights:\n",
      "kitten           1.066\n",
      "love             1.066\n",
      "sttrawberries    1.066\n",
      "kiwi             1.066\n",
      "smoothie         1.066\n",
      "Name: 0, dtype: float64\n",
      "----------\n",
      "For topic 1, here are the the top 5 words with weights:\n",
      "fluffy     1.061\n",
      "kittems    1.061\n",
      "pippies    1.061\n",
      "like       1.061\n",
      "mangos     1.061\n",
      "Name: 1, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print_topics(lda_model, vectorizer, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Bonus: LDA Under the Hood\n",
    "\n",
    "üéØ The goal of an LDA is to find topics across documents.\n",
    "\n",
    "The LDA converts the **vectorized documents** (= `document_term_matrix`) into two matrices:\n",
    "\n",
    "- `document_topic_mixture`\n",
    "- `topic_word_mixture`\n",
    "\n",
    "<br>\n",
    "\n",
    "0Ô∏è‚É£ Choose the number of topics you want to detect in your corpus of documents\n",
    "\n",
    "Example: $n_{components} = 2$  ‚Üí   $\\text{Topic 0}$  and $\\text{Topic 1}$\n",
    "\n",
    "<br>\n",
    " \n",
    "1Ô∏è‚É£ Randomly assign each word in each document to one of topics\n",
    "\n",
    "Example: The word \"mangos\" in $\\text{Document 0}$ is randomly assigned to $\\text{Topic 1}$\n",
    "\n",
    "<br>\n",
    " \n",
    "2Ô∏è‚É£ Go through every word and its topic assignment in each document\n",
    "\n",
    "(1) Document Mixture $p(topic \\ t | document \\ d) $\n",
    "‚Üí   how often a topic  $t$  occurs in a document $d$\n",
    " \n",
    "(2) Topic Mixture $p(word \\ w | topic \\ t)$ ‚Üí\n",
    "  how often the word $w$ occurs in the topic $t$\n",
    " \n",
    "(3) Update $p(word \\ w \\ with \\ topic \\ t)$ = $p( t | d) * p( w | t)$\n",
    "\n",
    "<br>\n",
    "\n",
    "üîÅ Go through multiple iterations of step 2Ô∏è‚É£\n",
    "\n",
    "<br>\n",
    "\n",
    "üöÄ Eventually, the topics will start making sense\n",
    "\n",
    "<br>\n",
    "\n",
    "**Document Mixture (of Topics)**\n",
    "\n",
    "- Computing $p(topic \\ t | document \\ d)$ for every topic and every document is called **document mixture**\n",
    "\n",
    "- The ideal document mixture for our example would be the following (*falsely assuming without verification that topic 0  = food and topic 1 = animals*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "      <th>topic_food</th>\n",
       "      <th>topic_animals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I like mangos and oranges</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Frogs and turtles live in ponds</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kittens and puppies are fluffy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I had a spinach and kiwi smoothie</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My kitten loves strawberries</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           documents  topic_food  topic_animals\n",
       "0          I like mangos and oranges         1.0            0.0\n",
       "1    Frogs and turtles live in ponds         0.0            1.0\n",
       "2     Kittens and puppies are fluffy         0.0            1.0\n",
       "3  I had a spinach and kiwi smoothie         1.0            0.0\n",
       "4       My kitten loves strawberries         0.5            0.5"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"documents\": [\n",
    "        \"I like mangos and oranges\",\n",
    "        \"Frogs and turtles live in ponds\",\n",
    "        \"Kittens and puppies are fluffy\",\n",
    "        \"I had a spinach and kiwi smoothie\",\n",
    "        \"My kitten loves strawberries\"\n",
    "    ],\n",
    "    \"topic_food\": [1.0, 0.0, 0.0, 1.0, 0.5],\n",
    "    \"topic_animals\": [0.0, 1.0, 1.0, 0.0, 0.5]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "document_topic_matrix_ideal = pd.DataFrame(data)\n",
    "\n",
    "document_topic_matrix_ideal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic Mixture (of Words)**\n",
    "\n",
    "- Computing $p(word \\ w | topic \\ t)$ for every word and every topic is called **topic mixture**\n",
    "- The ideal topic mixture for our example would be the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>like</th>\n",
       "      <th>mangos</th>\n",
       "      <th>oranges</th>\n",
       "      <th>frog</th>\n",
       "      <th>turtle</th>\n",
       "      <th>live</th>\n",
       "      <th>ponds</th>\n",
       "      <th>kitten</th>\n",
       "      <th>puppies</th>\n",
       "      <th>fluffy</th>\n",
       "      <th>spinach</th>\n",
       "      <th>kiwi</th>\n",
       "      <th>smoothie</th>\n",
       "      <th>love</th>\n",
       "      <th>strawberry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>topic_food</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>topic_animal</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          topic  like  mangos  oranges  frog  turtle  live  ponds  kitten  \\\n",
       "0    topic_food     1       1        1     0       0     0      0       0   \n",
       "1  topic_animal     1       0        0     1       1     1      1       1   \n",
       "\n",
       "   puppies  fluffy  spinach  kiwi  smoothie  love  strawberry  \n",
       "0        0       0        1     1         1     1           1  \n",
       "1        1       1        0     0         0     0           0  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = {\n",
    "    \"topic\": [\"topic_food\", \"topic_animal\"],\n",
    "    \"like\": [1, 1],\n",
    "    \"mangos\": [1, 0],\n",
    "    \"oranges\": [1, 0],\n",
    "    \"frog\": [0, 1],\n",
    "    \"turtle\": [0, 1],\n",
    "    \"live\": [0, 1],\n",
    "    \"ponds\": [0, 1],\n",
    "    \"kitten\": [0, 1],\n",
    "    \"puppies\": [0, 1],\n",
    "    \"fluffy\": [0, 1],\n",
    "    \"spinach\": [1, 0],\n",
    "    \"kiwi\": [1, 0],\n",
    "    \"smoothie\": [1, 0],\n",
    "    \"love\": [1, 0],\n",
    "    \"strawberry\": [1, 0]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "topic_word_matrix_ideal = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "topic_word_matrix_ideal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hands-On! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlptest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
