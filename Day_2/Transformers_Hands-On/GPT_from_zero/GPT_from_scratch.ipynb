{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT from scratch\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/toelt-llc/HSLU-NLP-Bootcamp/blob/main/Day_1/Transformers_Hands-On/GPT_from_zero/GPT_from_scratch.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nobody likes writing ticket descriptions ;)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will be building your own GPT-style model that generates realistic-sounding tickets so you won't have to spend any more time writing all those long descriptions (you can just generate them!) ğŸ˜®\n",
    "\n",
    "<img src=\"pics/smarter_not_harder.jpg\" width = \"300px\">\n",
    "\n",
    "This notebook will cover both theory and practice. Really take the time to understand __why__ we're doing what we're doing rather than just implementing the tasks. \n",
    "\n",
    "ğŸ‘‰ğŸ‘‰ğŸ‘‰ This is the biggest exercise of the day and for good reason - we'll be solidifying all of the core concepts from the lecture, so don't stress if it takes you most of the day. ğŸ‘ˆ ğŸ‘ˆ ğŸ‘ˆ \n",
    "\n",
    "We're going to go through each step of this slowly and methodically, but the broad strokes look like this:\n",
    "\n",
    "### 1ï¸âƒ£ Data Preprocessing ğŸ“Š\n",
    "\n",
    "Read in our tickets data, clean it and split it into training, testing, and validation tensorflow datasets. ğŸ§¹ğŸ”€\n",
    "\n",
    "### 2ï¸âƒ£ Vocabulary Creation ğŸ“š\n",
    "\n",
    "To start, we'll be using a simplistic TextVectorization layer from TensorFlow with a custom text standardization function to turn our models into tokens. We're also going to make sure we can translate between our tokens and our original text nice and smoothly. ğŸ—‚ï¸\n",
    "\n",
    "### 3ï¸âƒ£ Model Creation and Training ğŸ”¨\n",
    "\n",
    "Define the architecture of the text generation model using a Transformer-based approach. This is where a lot of the heavy lifting will get done ğŸ—ï¸\n",
    "\n",
    "### 4ï¸âƒ£ Text Generation ğŸ“ğŸ”®\n",
    "\n",
    "Finally, we'll define a callback function to generate sample text at the end of each epoch. Let your model's creativity shine! âœ¨ğŸŒŸ\n",
    "\n",
    "\n",
    "### ğŸ‰5ï¸âƒ£ ğŸ‰  Freestyle \n",
    "By now you'll be a pro at NLP, so in this section, you'' have the opportunity to take the code you've written, tidy it up and and then you can start playing around with your own datasets! Get ready to explore and have some fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need this library a little later so for now, run the cell below to install the `keras_nlp` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Data Preprocessing ğŸ“ŠÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below with ```tickets.txt``` file in a ```data/``` folder. It will load the txt file into a variable - the variable will just be one long string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/tickets.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 1000 characters of the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm developing a sentiment analysis model for customer reviews, but I'm struggling with handling domain-specific language or sarcasm. What are some techniques like domain adaptation, transfer learning, or using pre-trained language models such as BERT or GPT-3 that can help me improve sentiment analysis performance on such challenging data? --- I'm facing challenges in detecting and handling outliers in my numerical data. How can I use techniques like the interquartile range (IQR), Z-score, or robust statistical methods to identify outliers and decide whether to remove them or treat them differently in my analysis or modeling pipeline? --- I'm working on a collaborative filtering-based recommendation system, and I need guidance on handling cold start problems when dealing with new users or items with limited interaction data. How can I leverage techniques like content-based filtering, popularity-based recommendations, or hybrid approaches to address the cold start issue? --- I'm encoun\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text[:1000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each ticket is broken up by some dashes. Split the text on the dashes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tickets = text.split(\" --- \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first task is to add \" EOS \" to the end of each of the strings we've created. This will let our model know that it's hit the end of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm developing a sentiment analysis model for customer reviews, but I'm struggling with handling domain-specific language or sarcasm. What are some techniques like domain adaptation, transfer learning, or using pre-trained language models such as BERT or GPT-3 that can help me improve sentiment analysis performance on such challenging data? EOS \""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tickets = [sentence + \" EOS \" for sentence in tickets]\n",
    "\n",
    "tickets[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many tickets you have and how long the longest ticket description (__in words - not characters__)  is. Save that maximum length as a variable `max_len`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_len = max([len(ticket.split()) for ticket in tickets])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 372 tickets in the dataset\n",
      "The longest ticket description is 56 words (including the 'EOS' word)\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(tickets)} tickets in the dataset\")\n",
    "print(f\"The longest ticket description is {max_len} words (including the 'EOS' word)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we need to convert our nicely prepared collections of sentences into tokens. To do that we are going to use a simple [TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer. \n",
    "\n",
    "We'll talk more about advanced Tokenizing techniques later but - for now - let's instantiate a tokenizer layer called `tokenize_layer` that \"standardizes\" all of our sentences to lower case (we won't worry about punctuation for the time being), outputs integers and has a maximum sentence length equal to our ```max_len```. Check the docs or docstrings for guidance on how to achieve these steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=\"lower\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_len,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've instantiated, we need to call the ```adapt()``` method of the layer to our sentences (i.e. pass in our our ```tickets``` variable into the `vectorize_layer.adapt()` function). When this layer is adapted, it will analyze the dataset, determine the frequency of individual string values, and create a vocabulary from them. \n",
    "\n",
    "Then we can investigate our vocabulary using the [```get_vocabulary()```](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization#get_vocabulary) method.\n",
    "\n",
    "Assign the list produced to a variable called `vocab` and take a look at the first 10 in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'or', 'like', 'and', \"i'm\", 'eos', 'can', 'techniques', 'to']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vectorize_layer.adapt(tickets)\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "vocab[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1150"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell to create dictionary that will allow you to translate your tokens back to words:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_lookup = dict(zip(range(len(vocab)), vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try calling vectorizer layer on an example sentence of your choosing. (i.e. running `vectorize_layer(\"Try me\")`). Ensure you get a tensor out on the other side, filled with integers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentence_as_tokens = vectorize_layer(\"Working with deep learning models is the best\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then ensure you can translate the tokens back into words with your dictionary\n",
    "<details >\n",
    "<summary>Click for hint ğŸ‘‡</summary>\n",
    "<br>\n",
    "The vectorizer outputs a tensor which you'll need to convert back to a list of numbers if you want to loop through them. Try using <code>.numpy().tolist()</code> on your tensor.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['working', 'with', 'deep', 'learning', 'models', 'is', 'the', 'best', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "translated_back = [index_lookup[token] for token in sentence_as_tokens.numpy().tolist()]\n",
    "translated_back\n",
    "print(translated_back)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may get some `[UNK]` tokens if you try putting in a word that isn't included in our rather small vocabularly but don't worry!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you feel comfortable with the vectorizer, let's loop through all of the sentences and tokenize each one of them! Save this list as a variable ```all_tokenized``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(56,), dtype=int64, numpy=\n",
       "array([  5,  44,  12,  77,  62,  20,  11, 105, 477, 288,   5, 356,  29,\n",
       "        32, 751,  72,   2, 613,  41, 141, 196,   8,   3, 528, 804,  88,\n",
       "        93,   2,  18,  90,  72,  24, 131, 327, 440,   2, 714, 249,   7,\n",
       "        47,  46,  31,  77,  62,  92,  16, 131, 784,  63,   6,   0,   0,\n",
       "         0,   0,   0,   0])>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_tokenized = [vectorize_layer(sentence) for sentence in tickets]\n",
    "\n",
    "print(len(all_tokenized))\n",
    "\n",
    "all_tokenized[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have a ```list``` of 372 tensors with each tensor being 1-dimensional tensor that is 56 long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of our sentences as sequences of tokens, let's think hard about exactly what our X and y are going to be. 372 sentences does not seem like a lot of information to train our model on - it isn't and this is partly so that we can have faster training times for demonstration purposes. But we can also split our sentences down into smaller X, y pairs. \n",
    "\n",
    "Our model is supposed to predict the next word in a sentence, only knowing the words that it has up until that point. Below you can see the most obvious next-word training example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"pics/gpt_scratch_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are many other sets of Xs and ys that we can get out of our sentence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"pics/quick_brown_examples.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a sentence of 8 words, we have created 7 (ie. n - 1) X-y pairs! Is there a way we can think about implementing this more quickly? Let's consider replicating our sentence a total of 7 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = pics/quick_brown_examples_X.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're interested in just the parts underlined in red - what would be great would be if we could take a mask and ignore anything that wasn't desired. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = pics/quick_brown_examples_masking.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly what `tf.linalg.band()` (you saw this in the warm-up exercise!) is going to do for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's think about our ys - we could take one from each row of our tensors, but there's a more efficient way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = pics/quick_brown_examples_y.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our ys will just be our sentence `sequence[1:]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take this dummy tensor and perform the follow operations on it:\n",
    "1) Create a tensor of shape 55x56 that is just repeats of the original sequence with ```tf.tile()```. First, you'll need to use `tf.expand_dims` here so that you can `tile` in two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "929397d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_tensor = tf.range(0,56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Mask out the upper triangle of the tensor with ```tf.linalg.band_part()```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Create our corresponding ys from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When those steps work, fill out the function that will `return X, y` (where X is a tensor of shape (55,56) and y is of shape (55,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_y_creator(sequence_tensor):\n",
    "\n",
    "    tiled_sequence = tf.tile(tf.expand_dims(sequence_tensor, 0), [max_len - 1, 1])\n",
    "    X_s = tf.linalg.band_part(tiled_sequence, -1, 0)\n",
    "    y_s = sequence_tensor[1:]\n",
    "\n",
    "    return X_s, y_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to run the cell below without error: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = X_y_creator(all_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply this function to each element in our ```all_tokenized``` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = []\n",
    "ys = []\n",
    "for sequence in all_tokenized:\n",
    "\n",
    "    X, y = X_y_creator(sequence)\n",
    "    Xs.append(X)\n",
    "    ys.append(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to tidy up a little - we've been doing things quite inefficiently with all of our ```for``` loops but that's so we can understand every step of the process and apply things methodically. We have a list of tensors for our `X`s and a list of `y`s for our `y`s so let's use `tf.concat` to create our 20460 training examples (that got big quickly!). Call this new variable `X` with shape `(20460, 56)` shape and do the same concat process for your ys and save it in a variable `y` `(shape (20460,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = tf.concat(Xs, axis = 0)\n",
    "y = tf.concat(ys, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's one thing we haven't considered yet! A lot of our y values are just 0s because there is so much padding. Let's quickly create a boolean mask to figure out where our `ys` are __not__ zero and then only keep those examples from both our X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y != 0\n",
    "\n",
    "X = X[y != 0]\n",
    "\n",
    "y = y[y != 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've just dropped almost 5000 useless training examples. Check your X and y against the tests below to make sure you've ended up with the right shapes and value for your X and y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All sorted! That was a lot of work - and you'll only have to do this once - but it's crucial you understand what is going into our model and what is being predicted from the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get to the tricky part - building our model! As you will recall from the lecture - GPT style models are what we call \"decoder-only\". GPT decoder-only models work by leveraging the Transformer architecture, specifically the decoder component, to generate output sequences based on input sequences. Let's take a look at this diagram that shows the architecture of GPT-2:\n",
    "\n",
    "<img src = \"pics/GPT2.png\" width=\"400px\">\n",
    "\n",
    "\n",
    "Focus your attention on the left side of the diagram and you can easily visualize the journey that our words (tokens) take along their path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Positional Encoding and Word Embeddings: \n",
    "\n",
    "Our input to the model is the lovely tokens we've just prepared. We now need to do two things to our input tokens:  \n",
    "\n",
    "1) Give them a regular token embedding (as we did in our NLP tasks) and also give them a positional embedding. As a reminder, a token embedding means taking a token and embedding its meaning across however many ```embedding_dimensions``` we choose. \n",
    "\n",
    "\n",
    "2) Use positional encoding to clues about where each word is in the sentence to the model, and this positional encoding is simply added to the input embeddings. \n",
    "\n",
    "This means the model understands __both__ the relative positions of the tokens in the sequence __as well as__ what each word \"means\". Fortunately, we can use a `TokenAndPositionEmbedding()` layer from the `keras_nlp` library to do both of these at once! See the diagram below for a reminder on this step from the lecture: \n",
    "\n",
    "<img src = pics/positional_encoding_sketch.png width=300px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. The Transformer Block: \n",
    "\n",
    "Our embedded vectors now enter the Transformer block which is the __heart of the GPT architecture__. The attention mechanism here allows the model to attend to different positions in the input sequence when making predictions. The model learns the importance of each input token (now represented with its embeddings) by calculating attention weights, which reflect the token's relevance to other tokens in the sequence. \n",
    "\n",
    "This happens in a few steps - first we project our embedded vectors into Query, Key, and Value vectors. This can get a little more complex for multi-headed attention as you can see [here](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853) but once we have these matrices, we perform scaled dot-product attention by simply following this formula!\n",
    "\n",
    "<img src = \"pics/key-query-value.png\" width=900>\n",
    "\n",
    "Once we've done that, and we have updated our embeddings, we pass it through the final layers of the Transformer Block which just add some Dropout and LayerNormalization. \n",
    "\n",
    "The upshot of all of this is that we end up with updated vectors on the other side of our Transformer Block - each vector will still be 512 long, but they'll contain more information about their importance with respect to the task at hand (in our case predicting the next word)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Making a prediction: \n",
    "\n",
    "We then need to think very carefully about what our output is going to be. \n",
    "\n",
    "Remember - our `X` is all of the sentence up to a point and our `y` is the next word. What does this mean for prediction? Well essentially we have a massive classification problem in front of us. We need to pick the next word correctly, so how many choices do we have? \n",
    "\n",
    "Answer: as many words as we have in our vocabulary! Our output will be a Dense layer with as many neurons as we have words in our vocabulary. It will have a \"softmax\" activation function - which just means that it will essentially be predicting probabilities across all of our neurons that add to one. We want the next word to have a value as close to 1 as possible.\n",
    "\n",
    "View the process below:\n",
    "\n",
    "<img src = \"pics/prediction_png_flow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this like for us in terms of code? Well here is our define model function with a few holes in it:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03347e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_nlp.layers import TokenAndPositionEmbedding\n",
    "import keras, keras_nlp\n",
    "\n",
    "def create_model(max_sequence_length, vocab_size, embedding_dimension):\n",
    "\n",
    "    # 1. First up we define a layer that just grabs the inputs to our model\n",
    "    # We use the standard Input() layer and we know that each X going into\n",
    "    # our model is going to be 56 long!\n",
    "    inputs = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "\n",
    "    # 2. Next we give our tokens Positional and Regular Embeddings which is done\n",
    "    # by a nicely built layer that takes these arguments!\n",
    "    x = TokenAndPositionEmbedding(vocab_size,\n",
    "                                  max_sequence_length,\n",
    "                                  embedding_dimension,\n",
    "                                  mask_zero = True)(inputs)\n",
    "\n",
    "    # 3. This part we are going to define in a moment - don't worry\n",
    "    # about it for now - we'll come back to it!\n",
    "    x = TransformerBlock(num_heads=4,\n",
    "                         embed_dim=embedding_dimension,\n",
    "                         ff_dim=embedding_dimension * 4)(x)\n",
    "\n",
    "\n",
    "    # 4. This is just a regular Dropout layer that you've\n",
    "    # seen earlier in the week that helps our model avoid overfitting\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "\n",
    "\n",
    "    # 5. At this point in the model we'll have tensors with\n",
    "    # shape (batch_size, sequence_length, embedding_dimension) but\n",
    "    # we want to squish it down so we use GlobalAveragePooling1d. All\n",
    "    # this does is average elements across our sequence and\n",
    "    # squish them into a (batch_size, embedding_dimension) tensor.\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "\n",
    "    # 6. Finally we just need to have our \"classification\" layer\n",
    "    # which needs to be as large as our vocabulary size\n",
    "    outputs = layers.Dense(vocab_size, activation='softmax')(x)\n",
    "\n",
    "\n",
    "    # Now we just just stick our model together with the Functional API\n",
    "    # and compile with \"adam\" for our optimizer and\n",
    "    # \"sparse_categorical_crossentropy\" to compute the loss\n",
    "    # between our predicted labels and our actual labels.\n",
    "    # We'll talk about perplexity a little later.\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=[keras_nlp.metrics.Perplexity(), 'accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnaud/miniconda3/envs/nlptest/lib/python3.12/site-packages/keras/src/layers/layer.py:939: UserWarning: Layer 'transformer_block_2' (of type TransformerBlock) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Functional name=functional_3, built=True>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_model(50, 50, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the cell above you will get an error! Why? Well, because we haven't defined many of our layers yet and we still need to implement out TransformerBlock- this is the part where all the magic happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To code our Transformer Block - we'll need to code our attention mechanism first. Here's a reminder on what that looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/lectures/Transformers/key-query-value.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this layer is doing is projecting our embedded inputs into three matrices which are  - queries, keys, and values - and using the interaction between all three to give us better embeddings for our words. Let's break it down step by step:\n",
    "\n",
    "1. We multiply the Q matrix with a transposed version of the K matrix.\n",
    "2. We divide this by the square root of our model dimension\n",
    "3. We take the Softmax of final dimension of the matrix the to get the scaled scores\n",
    "4. We multiply these softmax scores by the V matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coded_attention(query, key, value):\n",
    "        # Step 1: Matrix multiply the query with the transpose of the key\n",
    "\n",
    "\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "\n",
    "\n",
    "        # Step 2: Divide this matrix by the square root of the hidden dimension\n",
    "        # In our case this dimension will be 512 (with the square root being 22.6).\n",
    "        # You will have to use tf.cast(22.6, tf.float32) so that the two matrices can interact\n",
    "\n",
    "\n",
    "\n",
    "        divider = tf.cast(22.6, tf.float32)\n",
    "        scaled_score = score / divider\n",
    "\n",
    "\n",
    "        # Step 3:\n",
    "        # Compute the softmax_scores - use tf.nn.softmax(scaled_score, axis = ?)\n",
    "        # Think about what dimension we should be using our softmax along -\n",
    "        # it'll need to be our last dimension!\n",
    "\n",
    "\n",
    "        softmax_scores = tf.nn.softmax(scaled_score, axis=-1)\n",
    "\n",
    "\n",
    "        # Step 4:\n",
    "        # Matrix multiply the weights matrix with the value matrix\n",
    "        # and set this to be your \"output\"\n",
    "\n",
    "\n",
    "        output = tf.matmul(softmax_scores, value)\n",
    "\n",
    "\n",
    "\n",
    "        # Return *both* the output and the softmax_scores as a tuple\n",
    "        return output, softmax_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to test your function with some dummy tensors:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[1.0001328, 1.1001327],\n",
       "        [1.0003097, 1.1003097]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[0.49933627, 0.5006637 ],\n",
       "        [0.49845132, 0.50154865]], dtype=float32)>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy tensor for query\n",
    "example_query = tf.constant([[0.1, 0.2],\n",
    "                     [0.3, 0.4]])\n",
    "\n",
    "# Dummy tensor for key\n",
    "example_key = tf.constant([[0.5, 0.6],\n",
    "                   [0.7, 0.8]])\n",
    "\n",
    "# Dummy tensor for value\n",
    "example_value = tf.constant([[0.9, 1.0],\n",
    "                     [1.1, 1.2]])\n",
    "# Test your function\n",
    "coded_attention(example_query, example_key, example_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's run, we can fold our hand-coded attention into our larger MultiAttentionHead and the - even larger - TransformerBlock. \n",
    "\n",
    "__If you want to to go through and understand each step of the block below, do so later__, but for now you can run the cell below to define the architecture then move on down - we're almost there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, Model, Sequential\n",
    "import keras_nlp\n",
    "\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        # Your coded attention function goes here\n",
    "        return coded_attention(query, key, value)\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, 56, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Here, we \"project\" into long query, key, value vectors\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # We rearrange the projections for each head\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "\n",
    "        # We perform attention on our QKV for our heads\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ffn = Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attention_output = self.attention(inputs)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need to do now is put it all together. The function below will stitch everything together for us! It's really only 5 layers (although - as we've just seen - one of them is quite complicated!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_length, vocab_size, embedding_dimension):\n",
    "\n",
    "    inputs = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    x = TokenAndPositionEmbedding(vocab_size,\n",
    "                                  max_sequence_length,\n",
    "                                  embedding_dimension,\n",
    "                                  mask_zero = True)(inputs)\n",
    "    x = TransformerBlock(num_heads=4,\n",
    "                         embed_dim=embedding_dimension,\n",
    "                         ff_dim=embedding_dimension)(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(vocab_size, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=[keras_nlp.metrics.Perplexity(), 'accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need to instantiate our model with:\n",
    "- `max_sequence_length` = Our longest sequence length\n",
    "- `vocab_size` = The number of unique words we had in our vocabulary\n",
    "- `embedding_dimension` = 512 (512 should work well for a dataset of this size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnaud/miniconda3/envs/nlptest/lib/python3.12/site-packages/keras/src/layers/layer.py:939: UserWarning: Layer 'transformer_block_3' (of type TransformerBlock) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = create_model(56, 1150, 512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at your model summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ token_and_position_embedding_4  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">617,472</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)     â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ transformer_block_3             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,577,984</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_2      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1150</span>)           â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">589,950</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_6 (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ token_and_position_embedding_4  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m512\u001b[0m)        â”‚       \u001b[38;5;34m617,472\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)     â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ transformer_block_3             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m512\u001b[0m)        â”‚     \u001b[38;5;34m1,577,984\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mTransformerBlock\u001b[0m)              â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m512\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling1d_2      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_12 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1150\u001b[0m)           â”‚       \u001b[38;5;34m589,950\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,785,406</span> (10.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,785,406\u001b[0m (10.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,785,406</span> (10.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,785,406\u001b[0m (10.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that is left to do now is fit your model with our `X` and `y`. A batch size of 32 should be good and 20 epochs. Now you can just sip on some tea, read an explanation on perplexity below, and wait while our model works its magic! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before fitting, we need to expand the dims of y to make it work\n",
    "# with the perplexity measure when using the latest version of tf.\n",
    "# This changes the shape from (n_samples, ) to (n_samples, 1).\n",
    "y = tf.expand_dims(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m533/533\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 364ms/step - accuracy: 0.0942 - loss: 5.0368 - perplexity: 166.0547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x187ceee70>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(X, y, batch_size=32, epochs = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details> \n",
    "    <summary> Click HERE for a Perplexity Explanation</summary>\n",
    "<br>\n",
    "Perplexity is a metric commonly used in the context of Language Models to evaluate the quality and performance of the model in predicting the next word or token in a sequence of words. It measures how well the language model assigns probabilities to a given sequence of words.\n",
    "\n",
    "Mathematically, perplexity is calculated using the concept of cross-entropy. The perplexity score is the exponential of the average cross-entropy per word in a given dataset. The formula for perplexity is as follows:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity} = \\exp\\left(-\\frac{{\\sum_{{i=1}}^{{N}} \\log(p(w_i))}}{{N}}\\right)\n",
    "$$\n",
    "\n",
    "where $N$ represents the total number of words in the dataset, and $(p(w_i))$ is the probability assigned by the language model to the $(i)$-th word in the sequence.\n",
    "\n",
    "The formula involves taking the logarithm of the model's predicted probabilities for each word in the sequence and summing them. Dividing this sum by the total number of words $(N)$ and then taking the exponential gives the perplexity score.\n",
    "\n",
    "A lower perplexity value indicates that the language model is more confident and accurate in its predictions, as it assigns higher probabilities to the true words in the dataset. Conversely, a higher perplexity score suggests that the model is more uncertain and less accurate in its predictions.\n",
    "\n",
    "Perplexity is often used to compare different language models or to track the progress of a model during training. Lower perplexity values are generally desired, indicating better language modeling performance.</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll remember that GPT-style models work simply by taking an input sequence, predicting the next word, adding it to our existing input sequence and then predicting again and again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to write a generate function that takes as its input the starter_string. \n",
    "\n",
    "Then - in a for loop of ``range(max_len - len(starter_string.split()))``:\n",
    "\n",
    "1. We use our ```vectorize_layer``` to convert it to a ```token_tensor```\n",
    "2. Use ```model.predict(token_tensor)``` to get out a tensor of size vocab_size out of our model (you'll need to use ```tf.expand_dims(token_tensor, 0)``` so that it looks like the right input size for our model!\n",
    "3. Use ```tf.argmax()``` to find the index of the most probable (largest number) word out (this is effectively a \"greedy\" algorithm as we just take the most likely word at any given step.\n",
    "4. Use our ```index_lookup``` dictionary from earlier to convert that index from a number back into a word\n",
    "5. Add that word to our input string and repeat UNLESS\n",
    "\n",
    "Two catches: \n",
    "\n",
    "1) If we predict the word \"eos\" that means our model has predicted the end of the sentence so we ```return``` our ```starter_string``` in its current form. \n",
    "\n",
    "2) If the length of our string (when split on whitespace) is 55 then we also ```return``` our ```starter_string``` in its current form. \n",
    "\n",
    "N.B. Make sure you add whitespace when you add the word to your sequence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(starter_string):\n",
    "\n",
    "\n",
    "    for x in range(max_len - len(starter_string.split())):\n",
    "        tokens = vectorize_layer(starter_string)\n",
    "        token_expanded = tf.expand_dims(tokens, 0)\n",
    "        pred = model.predict(token_expanded)\n",
    "        index_pred = pred.argmax()\n",
    "        word = index_lookup[index_pred]\n",
    "        print(word)\n",
    "        if word == \"eos\":\n",
    "            return starter_string\n",
    "        if len(starter_string.split())==55:\n",
    "            return starter_string\n",
    "        else:\n",
    "            starter_string += f\" {word} \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311ms/step\n",
      "facing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "facing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "facing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "facing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "facing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "facing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "facing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "facing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "facing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "facing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "facing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "handling\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "handling\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "handling\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "handling\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "handling\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "imbalanced\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "imbalanced\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step\n",
      "imbalanced\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "imbalanced\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "imbalanced\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "imbalanced\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "imbalanced\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "imbalanced\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "imbalanced\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "data\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "like\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "or\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "can\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "effectively\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "and\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "and\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "and\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step\n",
      "represent\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "represent\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "represent\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "represent\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "represent\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "represent\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "represent\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "represent\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "represent\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "represent\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "represent\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "represent\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "represent\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "represent\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "represent\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "represent\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "modeling?\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "eos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I need facing  facing  facing  facing  facing  facing  facing  facing  facing  facing  facing  handling  handling  handling  handling  handling  imbalanced  imbalanced  imbalanced  imbalanced  imbalanced  imbalanced  imbalanced  imbalanced  imbalanced  data  like  or  can  effectively  and  and  and  represent  represent  represent  represent  represent  represent  represent  represent  represent  represent  represent  represent  represent  represent  represent  represent  modeling? '"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"I need\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations!!! You've just built your own GPT model from first principles :)\n",
    "\n",
    "Naturally there are __much__ more efficient ways to do what we've just done - you will almost never end up writing your own Transformer block, attention mechanism or even full GPT model from scratch. HuggingFace abstracts so much of the difficult coding away from us, which is why fine-tuning existing models is much more effective. \n",
    "\n",
    "The dataset we've been working with has been very, very small and we have done very simplistic tokenization too (each word is currently assigned its own unique token and punctuation is kept in as well, but if you'd like to try working with some more messy data, you can tidy up your code into Python files and then repeat your steps on the real data found in these 10000 StackOverflow answers (answers.csv) and build a more complex generator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlptest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
